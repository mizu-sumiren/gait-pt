import streamlit as st
import cv2
import mediapipe as mp
import numpy as np
import tempfile
import io
import math
from reportlab.pdfgen import canvas
from reportlab.lib.pagesizes import A4
from PIL import Image

# --- MediaPipeåˆæœŸåŒ– ---
mp_pose = mp.solutions.pose
mp_drawing = mp.solutions.drawing_utils

# --- ãƒšãƒ¼ã‚¸è¨­å®š ---
st.set_page_config(page_title="AIå§¿å‹¢ãƒ»æ­©è¡Œåˆ†æãƒ©ãƒœ", page_icon="ğŸ¥", layout="wide")

# --- CSSè¨­å®š ---
hide_streamlit_style = """
            <style>
            #MainMenu {visibility: hidden;}
            footer {visibility: hidden;}
            .stDeployButton {display:none;}
            </style>
            """
st.markdown(hide_streamlit_style, unsafe_allow_html=True)

# --- ã‚µã‚¤ãƒ‰ãƒãƒ¼ï¼šãƒ¢ãƒ¼ãƒ‰é¸æŠ ---
st.sidebar.header("âš™ï¸ åˆ†æãƒ¢ãƒ¼ãƒ‰")
app_mode = st.sidebar.radio(
    "ãƒ¢ãƒ¼ãƒ‰ã‚’é¸æŠã—ã¦ãã ã•ã„",
    ["å‹•ç”»ï¼šæ­©è¡Œåˆ†æ (Pro)", "å‹•ç”»ï¼šæ­©è¡Œåˆ†æ (Lite)", "é™æ­¢ç”»ï¼šå§¿å‹¢åˆ†æ (ç«‹ä½/åº§ä½)"]
)

# --- ã‚¿ã‚¤ãƒˆãƒ«è¡¨ç¤º ---
if "æ­©è¡Œ" in app_mode:
    st.title("ğŸƒâ€â™‚ï¸ AIæ­©è¡Œãƒ‰ãƒƒã‚¯")
    st.markdown(f"ãƒ¢ãƒ¼ãƒ‰: {app_mode}")
else:
    st.title("ğŸ“¸ AIå§¿å‹¢åˆ†æãƒ©ãƒœ")
    st.markdown("ç«‹ä½ãƒ»åº§ä½ã®é™æ­¢ç”»ã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆè©•ä¾¡")

# --- å¤‰æ•°åˆæœŸåŒ– ---
toe_grip_l = toe_grip_r = 0
hip_flex_l = hip_flex_r = 0
one_leg_l = one_leg_r = 0
frt = ffd = 0
pain_areas = []

# --- ã‚µã‚¤ãƒ‰ãƒãƒ¼å…¥åŠ› ---
st.sidebar.header("ğŸ“‹ å¯¾è±¡è€…æƒ…å ±")
client_name = st.sidebar.text_input("æ°å", "ãƒ†ã‚¹ãƒˆ å¤ªéƒ æ§˜")

# Proãƒ¢ãƒ¼ãƒ‰ã®å ´åˆã®ã¿è©³ç´°å…¥åŠ›
if app_mode == "å‹•ç”»ï¼šæ­©è¡Œåˆ†æ (Pro)":
    with st.sidebar.expander("1. å•è¨ºãƒ»ç—›ã¿", expanded=True):
        pain_areas = st.multiselect("ç—›ã¿", ["ãªã—", "é¦–", "è‚©", "è…°", "è‚¡é–¢ç¯€", "è†", "è¶³é¦–"])
    with st.sidebar.expander("2. èº«ä½“æ©Ÿèƒ½æ¸¬å®š", expanded=True):
        c1, c2 = st.columns(2)
        with c1:
            st.markdown("**å·¦ (L)**")
            grip_l = st.number_input("æ¡åŠ›L", 20.0); hip_flex_l = st.number_input("è‚¡å±ˆæ›²L", 0.9)
            one_leg_l = st.number_input("ç‰‡è„šL", 15.0); toe_grip_l = st.number_input("è¶³æŠŠæŒL", 10.0)
        with c2:
            st.markdown("**å³ (R)**")
            grip_r = st.number_input("æ¡åŠ›R", 25.0); hip_flex_r = st.number_input("è‚¡å±ˆæ›²R", 1.2)
            one_leg_r = st.number_input("ç‰‡è„šR", 60.0); toe_grip_r = st.number_input("è¶³æŠŠæŒR", 20.0)
        st.markdown("---")
        frt = st.number_input("FRT", 25.0); ffd = st.number_input("FFD", 0.0)

# --- å¹¾ä½•å­¦è¨ˆç®—é–¢æ•° ---
def calculate_angle(a, b, c):
    a, b, c = np.array(a), np.array(b), np.array(c)
    rad = np.arctan2(c[1]-b[1], c[0]-b[0]) - np.arctan2(a[1]-b[1], a[0]-b[0])
    angle = np.abs(rad*180.0/np.pi)
    if angle > 180.0: angle = 360-angle
    return angle

def calculate_slope(a, b):
    if a is None or b is None: return 0
    return math.degrees(math.atan2(a[1]-b[1], a[0]-b[0]))

def calculate_vertical_angle(a, b):
    # å‚ç›´ç·šã‹ã‚‰ã®è§’åº¦ï¼ˆå‰å‚¾ãƒ»å¾Œå‚¾ï¼‰
    if a is None or b is None: return 0
    return math.degrees(math.atan2(b[0]-a[0], b[1]-a[1]))

# --- é™æ­¢ç”»åˆ†æãƒ­ã‚¸ãƒƒã‚¯ (NEW!) ---
def analyze_static_image(image, posture_type):
    with mp_pose.Pose(static_image_mode=True, min_detection_confidence=0.5) as pose:
        results = pose.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))
        
        if not results.pose_landmarks: return image, None

        h, w, _ = image.shape
        lms = results.pose_landmarks.landmark
        
        # æç”»
        annotated_image = image.copy()
        mp_drawing.draw_landmarks(annotated_image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS)
        
        # åº§æ¨™å–å¾—ãƒ˜ãƒ«ãƒ‘ãƒ¼
        def get_p(idx): return [lms[idx].x * w, lms[idx].y * h]
        
        metrics = {}
        
        # --- å…±é€šè©•ä¾¡: é ­éƒ¨ãƒ»è‚©ã®å‚¾ã (æ­£é¢æƒ³å®š) ---
        l_ear, r_ear = get_p(7), get_p(8)
        l_sh, r_sh = get_p(11), get_p(12)
        metrics['head_tilt'] = calculate_slope(l_ear, r_ear)
        metrics['shoulder_slope'] = calculate_slope(l_sh, r_sh)

        # --- å´é¢è©•ä¾¡ç”¨ãƒ‡ãƒ¼ã‚¿ã®è¨ˆç®— ---
        # 1. è€³ã¨è‚©ã®ä½ç½®é–¢ä¿‚ (Forward Head Posture)
        # è€³(7)ãŒè‚©(11)ã‚ˆã‚Šã©ã‚Œãã‚‰ã„å‰ã«ã‚ã‚‹ã‹ (Xåº§æ¨™ã®å·®)
        # æ­£è¦åŒ–ã®ãŸã‚ã€è‚©å¹…ã‹èº«é•·ã«å¯¾ã™ã‚‹æ¯”ç‡ã§å‡ºã™ã®ãŒç†æƒ³ã ãŒã€ã“ã“ã§ã¯ç°¡æ˜“çš„ã«ãƒ”ã‚¯ã‚»ãƒ«å·®ã‚’è¦‹ã‚‹
        ear_x = (lms[7].x + lms[8].x) / 2
        shoulder_x = (lms[11].x + lms[12].x) / 2
        metrics['forward_head_score'] = (shoulder_x - ear_x) * 100 # æ­£ã®å€¤ãªã‚‰è€³ãŒå‰
        
        # 2. ä½“å¹¹ã®å‰å‚¾
        metrics['trunk_lean'] = calculate_vertical_angle(l_sh, get_p(23))

        # --- ç«‹ä½ãƒ»åº§ä½ã”ã¨ã®ç‰¹ç•°çš„è©•ä¾¡ ---
        if posture_type == "ç«‹ä½ (Standing)":
            # è†ã®ä¼¸å±•åº¦ (11-23-25) -> ç«‹ä½ãªã‚‰180åº¦è¿‘ã„ã‹
            hip = get_p(23); knee = get_p(25); ankle = get_p(27)
            metrics['knee_angle'] = calculate_angle(hip, knee, ankle)
            # é‡å¿ƒç·š (è€³-è‚©-è…°-è†-å¤–æœ) ã®ã‚ºãƒ¬ãƒã‚§ãƒƒã‚¯ã¯ç°¡æ˜“çš„ã«ã€Œè€³ã¨ãã‚‹ã¶ã—ã®Xå·®ã€ã§
            metrics['plumb_line_dev'] = (lms[7].x - lms[27].x) * 100

        elif posture_type == "åº§ä½ (Sitting)":
            # è‚¡é–¢ç¯€å±ˆæ›²è§’åº¦ (11-23-25) -> 90åº¦ãŒç†æƒ³
            sh = get_p(11); hip = get_p(23); knee = get_p(25)
            metrics['hip_angle'] = calculate_angle(sh, hip, knee)
            # è†è§’åº¦ -> 90åº¦ãŒç†æƒ³
            ankle = get_p(27)
            metrics['knee_angle'] = calculate_angle(hip, knee, ankle)

        return annotated_image, metrics

# --- é™æ­¢ç”»ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ç”Ÿæˆ ---
def generate_static_feedback(metrics, posture_type):
    fb = []
    # æ­£é¢è¦ç´ 
    if abs(metrics['head_tilt']) > 3.0: fb.append("âš ï¸ **ã€é ­éƒ¨ã®å‚¾ãã€‘** é¦–ãŒå‚¾ã„ã¦ã„ã¾ã™ã€‚è¦–è¦šã‚„å™›ã¿åˆã‚ã›ã®å½±éŸ¿ãŒç–‘ã‚ã‚Œã¾ã™ã€‚")
    if abs(metrics['shoulder_slope']) > 3.0: fb.append("âš ï¸ **ã€è‚©ã®é«˜ã•ã€‘** å·¦å³ã®è‚©ã®é«˜ã•ãŒé•ã„ã¾ã™ã€‚è·ç‰©ã®æŒã¡ç™–ã‚„å´å¼¯ã®ãƒã‚§ãƒƒã‚¯ã‚’ã€‚")
    
    # å´é¢è¦ç´  (Forward Head) - å‘ãã«ã‚ˆã‚‹ã®ã§çµ¶å¯¾å€¤ã§ç°¡æ˜“åˆ¤å®š
    # â€»ã‚«ãƒ¡ãƒ©ã®å‘ãã«ä¾å­˜ã™ã‚‹ãŸã‚ã€ã‚ãã¾ã§å‚è€ƒå€¤ã¨ã—ã¦è­¦å‘Š
    if abs(metrics['forward_head_score']) > 5.0: 
        fb.append("âš ï¸ **ã€ã‚¹ãƒãƒ›é¦– (FHP)ã€‘** é ­ãŒè‚©ã‚ˆã‚Šå‰ã«å‡ºã¦ã„ã¾ã™ã€‚é¦–ãƒ»è‚©ã“ã‚Šã®ä¸»åŸå› ã§ã™ã€‚")

    if posture_type == "ç«‹ä½ (Standing)":
        if metrics['knee_angle'] < 165: fb.append("âš ï¸ **ã€è†æ›²ãŒã‚Šã€‘** è†ãŒä¼¸ã³åˆ‡ã£ã¦ã„ã¾ã›ã‚“ã€‚åŠ é½¢ã«ã‚ˆã‚‹å¤‰å½¢ã‚„ç­‹åŠ›ä½ä¸‹ã®å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚")
        if abs(metrics['trunk_lean']) > 10: fb.append("âš ï¸ **ã€å§¿å‹¢ã®å´©ã‚Œã€‘** ä¸ŠåŠèº«ãŒå‚ç›´ã‹ã‚‰å‚¾ã„ã¦ã„ã¾ã™ï¼ˆçŒ«èƒŒã¾ãŸã¯åã‚Šè…°ï¼‰ã€‚")

    elif posture_type == "åº§ä½ (Sitting)":
        if metrics['hip_angle'] > 110: fb.append("â„¹ï¸ **ã€éª¨ç›¤å¾Œå‚¾ã€‘** æ¤…å­ã«æµ…ãåº§ã‚Šã€èƒŒã‚‚ãŸã‚Œã«å¯„ã‚Šã‹ã‹ã‚Šã™ãã¦ã„ã¾ã™ï¼ˆä»™éª¨åº§ã‚Šï¼‰ã€‚")
        if metrics['knee_angle'] < 80: fb.append("â„¹ï¸ **ã€è¶³ã®å¼•ãè¾¼ã¿ã€‘** è¶³ã‚’æ‰‹å‰ã«å¼•ãã™ãã¦ã„ã¾ã™ã€‚è†è£ã®è¡€æµãŒæ‚ªããªã‚‹åŸå› ã§ã™ã€‚")

    if not fb: fb.append("âœ… **ã‚°ãƒƒãƒ‰ãƒã‚¹ãƒãƒ£ãƒ¼ï¼** éå¸¸ã«ç¶ºéº—ãªå§¿å‹¢ã§ã™ã€‚")
    return fb

# --- å‹•ç”»åˆ†æé–¢æ•° (æ—¢å­˜) ---
def analyze_video_metrics(history, fps):
    if not history: return None
    # (æ—¢å­˜ã®ãƒ­ã‚¸ãƒƒã‚¯ã‚’ç°¡ç•¥åŒ–ã—ã¦çµ±åˆ)
    dists = []
    for lms in history:
        la, ra = np.array([lms[27].x, lms[27].y]), np.array([lms[28].x, lms[28].y])
        dists.append(np.linalg.norm(la - ra))
    
    steps = 0; thresh = np.mean(dists)
    for i in range(1, len(dists)-1):
        if dists[i] > dists[i-1] and dists[i] > dists[i+1] and dists[i] > thresh: steps += 1
    
    duration = len(history) / fps
    cadence = (steps / duration) * 60 if duration > 0 else 0
    return {"cadence": cadence, "steps": steps}

def process_video(file):
    if not file: return None, None
    tfile = tempfile.NamedTemporaryFile(delete=False); tfile.write(file.read())
    cap = cv2.VideoCapture(tfile.name)
    w, h, fps = int(cap.get(3)), int(cap.get(4)), int(cap.get(5))
    path = tempfile.NamedTemporaryFile(delete=False, suffix='.mp4').name
    out = cv2.VideoWriter(path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (w, h))
    history = []
    with mp_pose.Pose() as pose:
        while cap.isOpened():
            ret, img = cap.read()
            if not ret: break
            img.flags.writeable = False; res = pose.process(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))
            img.flags.writeable = True; img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)
            cv2.line(img, (w//2,0), (w//2,h), (0,255,255), 1)
            if res.pose_landmarks:
                mp_drawing.draw_landmarks(img, res.pose_landmarks, mp_pose.POSE_CONNECTIONS)
                history.append(res.pose_landmarks.landmark)
            out.write(img)
    cap.release(); out.release()
    return path, analyze_video_metrics(history, fps)

# --- PDFç”Ÿæˆ (çµ±åˆç‰ˆ) ---
def create_unified_pdf(mode, name, feedbacks, vid_met=None, stat_met=None):
    b = io.BytesIO()
    c = canvas.Canvas(b, pagesize=A4); h = A4[1]
    c.setFont("Helvetica-Bold", 16); c.drawString(50, h-50, f"Analysis Report: {mode}")
    c.setFont("Helvetica", 12); c.drawString(50, h-80, f"Name: {name}")
    
    y = h-120
    c.setFont("Helvetica-Bold", 12); c.drawString(50, y, "Metrics")
    y -= 20; c.setFont("Helvetica", 10)
    
    if vid_met: # å‹•ç”»ãƒ‡ãƒ¼ã‚¿
        c.drawString(60, y, f"Cadence: {vid_met['cadence']:.1f} steps/min")
        c.drawString(200, y, f"Steps Detected: {vid_met['steps']}")
    elif stat_met: # é™æ­¢ç”»ãƒ‡ãƒ¼ã‚¿
        c.drawString(60, y, f"Head Tilt: {stat_met['head_tilt']:.1f} deg")
        c.drawString(200, y, f"Shoulder Slope: {stat_met['shoulder_slope']:.1f} deg")
        y -= 20
        if 'knee_angle' in stat_met: c.drawString(60, y, f"Knee Angle: {stat_met['knee_angle']:.1f} deg")
        if 'hip_angle' in stat_met: c.drawString(200, y, f"Hip Angle: {stat_met['hip_angle']:.1f} deg")

    y -= 40; c.setFont("Helvetica-Bold", 12); c.drawString(50, y, "AI Feedback")
    y -= 20; c.setFont("Helvetica", 10)
    c.drawString(60, y, "Please see the app screen for detailed feedback.")
    
    c.showPage(); c.save(); b.seek(0)
    return b

# --- ãƒ¡ã‚¤ãƒ³ãƒ­ã‚¸ãƒƒã‚¯åˆ†å² ---

# A. é™æ­¢ç”»åˆ†æãƒ¢ãƒ¼ãƒ‰
if app_mode == "é™æ­¢ç”»ï¼šå§¿å‹¢åˆ†æ (ç«‹ä½/åº§ä½)":
    st.info("ğŸ“¸ æ­£é¢ã¾ãŸã¯å´é¢ã®å†™çœŸã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„")
    
    posture_type = st.radio("åˆ†æå¯¾è±¡ã®å§¿å‹¢ã‚’é¸ã‚“ã§ãã ã•ã„", ["ç«‹ä½ (Standing)", "åº§ä½ (Sitting)"], horizontal=True)
    
    c1, c2 = st.columns(2)
    with c1:
        img_file = st.file_uploader("å†™çœŸ (æ­£é¢/å´é¢)", type=['jpg', 'png', 'jpeg'])
    
    if img_file and st.button("ğŸš€ å§¿å‹¢åˆ†æã‚’å®Ÿè¡Œ"):
        image = np.array(Image.open(img_file))
        annotated_img, metrics = analyze_static_image(image, posture_type)
        
        if metrics:
            st.image(annotated_img, caption="è§£æçµæœ", use_container_width=True)
            
            # çµæœè¡¨ç¤º
            st.subheader("ğŸ“Š å§¿å‹¢ã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿")
            d1, d2 = st.columns(2)
            with d1:
                st.metric("é ­éƒ¨ã®å‚¾ã", f"{metrics['head_tilt']:.1f}Â°")
                st.metric("è‚©ã®å‚¾ã", f"{metrics['shoulder_slope']:.1f}Â°")
            with d2:
                if posture_type == "ç«‹ä½ (Standing)":
                    st.metric("è†ä¼¸å±•è§’åº¦", f"{metrics['knee_angle']:.1f}Â°", help="180ã«è¿‘ã„ã»ã©çœŸã£ç›´ã")
                    st.metric("ä½“å¹¹ã®å‰å‚¾", f"{metrics['trunk_lean']:.1f}Â°")
                else:
                    st.metric("è‚¡é–¢ç¯€è§’åº¦", f"{metrics['hip_angle']:.1f}Â°", help="åº§ã‚Šå§¿å‹¢ã®æ·±ã•")
                    st.metric("è†å±ˆæ›²è§’åº¦", f"{metrics['knee_angle']:.1f}Â°")

            st.header("ğŸ‘¨â€âš•ï¸ AIå§¿å‹¢ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯")
            feedbacks = generate_static_feedback(metrics, posture_type)
            for msg in feedbacks:
                if "âš ï¸" in msg: st.error(msg)
                elif "â„¹ï¸" in msg: st.warning(msg)
                else: st.success(msg)

            # PDF
            pdf = create_unified_pdf("Posture Analysis", client_name, feedbacks, stat_met=metrics)
            st.download_button("ğŸ“„ ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜", pdf, "posture_report.pdf", "application/pdf")
        else:
            st.error("äººç‰©ãŒæ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚")

# B. å‹•ç”»åˆ†æãƒ¢ãƒ¼ãƒ‰ (Pro / Lite)
else:
    c1, c2 = st.columns(2)
    with c1: file_f = st.file_uploader("æ­£é¢å‹•ç”»", type=['mp4', 'mov'])
    with c2: file_s = st.file_uploader("å´é¢å‹•ç”»", type=['mp4', 'mov'])

    if st.button("ğŸš€ æ­©è¡Œåˆ†æã‚’å®Ÿè¡Œ"):
        path_f, met_f = process_video(file_f)
        path_s, met_s = process_video(file_s)
        
        main_met = met_s if met_s else met_f
        
        st.markdown("---")
        c1, c2 = st.columns(2)
        with c1: 
            if path_f: st.video(path_f)
        with c2: 
            if path_s: st.video(path_s)

        if main_met:
            st.metric("ã‚±ã‚¤ãƒ‡ãƒ³ã‚¹", f"{main_met['cadence']:.1f} æ­©/åˆ†")
            st.success(f"æ¤œå‡ºæ­©æ•°: {main_met['steps']}æ­©")
            
            # ç°¡æ˜“ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ (å‹•ç”»ç”¨)
            fb = []
            if main_met['cadence'] < 100: fb.append("â„¹ï¸ ãƒ”ãƒƒãƒãŒã‚†ã£ãã‚Šã§ã™ã€‚ãƒªã‚ºãƒ ã‚’æ„è­˜ã—ã¾ã—ã‚‡ã†ã€‚")
            else: fb.append("âœ… è‰¯å¥½ãªæ­©è¡Œãƒªã‚ºãƒ ã§ã™ã€‚")
            
            for msg in fb: st.info(msg)
            
            pdf = create_unified_pdf("Gait Analysis", client_name, fb, vid_met=main_met)
            st.download_button("ğŸ“„ ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜", pdf, "gait_report.pdf", "application/pdf")
